{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Variational Autoencoders (CVAEs)\n",
    "\n",
    "This notebook follows the Pyro tutorial on CVAEs (https://pyro.ai/examples/cvae.html).\n",
    "\n",
    "CVAEs extend VAEs by conditioning on an output label y. This allows the use of VAE machinery in structured prediction tasks. The CVAE is composed of several multilayer perceptrons: the conditional prior network p(z|x), the recognition network q(z|x,y), and the generation network p(y|x,z).\n",
    "\n",
    "The CVAE can also be conceptualized in other ways, such as enforcing constraints c rather than generating predictions y. In this case, we may have the MLPs p(z|c), p(x|z,c), and q(z|x,c).\n",
    "\n",
    "In this notebook, we will follow the first formulation. Our goal will be to predict an MNIST digit given only the bottom-left quadrant of the image. We will see that the CVAE offers an advantage over traditional neural networks, which can only make a single prediction. In some cases, the true digit will be genuinely uncertain given only the bottom-left quadrant. A traditional NN would give a blurred combination of the possible digits. The CVAE, on the other hand, will reflect the uncertainty, because we draw images from its posterior distribution, so that we will get a mix of clear digit images, with the mix proportions reflecting their posterior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "First we will load the MNIST data. Then we will implement a simple baseline MLP to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, functional\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.contrib.examples.util import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes and functions for loading MNIST data and preparing for CVAE\n",
    "\n",
    "# MNIST dataset loader to load data for CVAE prediction task\n",
    "class CVAEMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, download=False):\n",
    "        self.original = MNIST(root, train=train, download=download)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, digit = self.original[item]\n",
    "        sample = {'original': image, 'digit': digit}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# helper function to convert the data to pytorch tensors \n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        sample['original'] = functional.to_tensor(sample['original'])\n",
    "        sample['digit'] = torch.as_tensor(np.asarray(sample['digit']),\n",
    "                                          dtype=torch.int64)\n",
    "        return sample\n",
    "\n",
    "# class to mask quadrants of MNIST images for our prediction task\n",
    "class MaskImages:\n",
    "    \"\"\"This torchvision image transformation prepares the MNIST digits to be\n",
    "    used in the tutorial. Depending on the number of quadrants to be used as\n",
    "    inputs (1, 2, or 3), the transformation masks the remaining (3, 2, 1)\n",
    "    quadrant(s) setting their pixels with -1. Additionally, the transformation\n",
    "    adds the target output in the sample dict as the complementary of the input\n",
    "    \"\"\"\n",
    "    def __init__(self, num_quadrant_inputs, mask_with=-1):\n",
    "        if num_quadrant_inputs <= 0 or num_quadrant_inputs >= 4:\n",
    "            raise ValueError('Number of quadrants as inputs must be 1, 2 or 3')\n",
    "        self.num = num_quadrant_inputs\n",
    "        self.mask_with = mask_with\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        tensor = sample['original'].squeeze()\n",
    "        out = tensor.detach().clone()\n",
    "        h, w = tensor.shape\n",
    "\n",
    "        # removes the bottom left quadrant from the target output\n",
    "        out[h // 2:, :w // 2] = self.mask_with\n",
    "        # if num of quadrants to be used as input is 2,\n",
    "        # also removes the top left quadrant from the target output\n",
    "        if self.num == 2:\n",
    "            out[:, :w // 2] = self.mask_with\n",
    "        # if num of quadrants to be used as input is 3,\n",
    "        # also removes the top right quadrant from the target output\n",
    "        if self.num == 3:\n",
    "            out[:h // 2, :] = self.mask_with\n",
    "\n",
    "        # now, sets the input as complementary\n",
    "        inp = tensor.clone()\n",
    "        inp[out != -1] = self.mask_with\n",
    "\n",
    "        sample['input'] = inp\n",
    "        sample['output'] = out\n",
    "        return sample\n",
    "\n",
    "\n",
    "# function to load, prepare, and mask data using the classes above\n",
    "def get_data(num_quadrant_inputs, batch_size):\n",
    "    transforms = Compose([\n",
    "        ToTensor(),\n",
    "        MaskImages(num_quadrant_inputs=num_quadrant_inputs)\n",
    "    ])\n",
    "    datasets, dataloaders, dataset_sizes = {}, {}, {}\n",
    "    for mode in ['train', 'val']:\n",
    "        datasets[mode] = CVAEMNIST(\n",
    "            '../data',\n",
    "            download=True,\n",
    "            transform=transforms,\n",
    "            train=mode == 'train'\n",
    "        )\n",
    "        dataloaders[mode] = DataLoader(\n",
    "            datasets[mode],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=mode == 'train',\n",
    "            num_workers=0\n",
    "        )\n",
    "        dataset_sizes[mode] = len(datasets[mode])\n",
    "\n",
    "    return datasets, dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline network to compare to -- simple feedforward MLP\n",
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self, hidden_1, hidden_2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        hidden = self.relu(self.fc2(hidden))\n",
    "        y = torch.sigmoid(self.fc3(hidden))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a modified binary cross entropy loss, which only computes the loss for non-masked pixels\n",
    "class MaskedBCELoss(nn.Module):\n",
    "    def __init__(self, masked_with=-1):\n",
    "        super().__init__()\n",
    "        self.masked_with = masked_with\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(input.shape)\n",
    "        loss = F.binary_cross_entropy(input, target, reduction='none')\n",
    "        loss[target == self.masked_with] = 0\n",
    "        return loss.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE Implementation\n",
    "\n",
    "The CVAE doesn't look too different from the VAE (see vae.ipynb) in Pyro. The conditional prior network p(z|x) and the recognition network q(z|x,y) are both like VAE Encoders, and the generation network p(y|x,z) is like a VAE decoder.\n",
    "\n",
    "We first define the Encoder and Decoder, and then the CVAE with those modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_1, hidden_2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc31 = nn.Linear(hidden_2, z_dim)  # for learning mean\n",
    "        self.fc32 = nn.Linear(hidden_2, z_dim)  # for learning covariance\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # put x and y together in the same image for simplification\n",
    "        xc = x.clone()\n",
    "        xc[x == -1] = y[x == -1]\n",
    "        xc = xc.view(-1, 784)\n",
    "        # then compute the hidden units\n",
    "        hidden = self.relu(self.fc1(xc))\n",
    "        hidden = self.relu(self.fc2(hidden))\n",
    "        # then return a mean vector and a positive square root covariance\n",
    "        # each of size batch_size * z_dim\n",
    "        z_loc = self.fc31(hidden)\n",
    "        z_scale = torch.exp(self.fc32(hidden))\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_1, hidden_2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        y = self.relu(self.fc1(z))\n",
    "        y = self.relu(self.fc2(y))\n",
    "        y = torch.sigmoid(self.fc3(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we define the CVAE class\n",
    "# it has the prior, generation, and recognition modules, \n",
    "#   which are of class Encoder, Decoder, and Encoder, respectively\n",
    "# it also includes a pre-trained baseline network for comparison and as a prior\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_1, hidden_2, pre_trained_baseline_net):\n",
    "        super().__init__()\n",
    "        # The CVAE is composed of multiple MLPs, such as recognition network\n",
    "        # qφ(z|x, y), (conditional) prior network pθ(z|x), and generation\n",
    "        # network pθ(y|x, z). Also, CVAE is built on top of the NN: not only\n",
    "        # the direct input x, but also the initial guess y_hat made by the NN\n",
    "        # are fed into the prior network.\n",
    "        self.baseline_net = pre_trained_baseline_net\n",
    "        self.prior_net = Encoder(z_dim, hidden_1, hidden_2)\n",
    "        self.generation_net = Decoder(z_dim, hidden_1, hidden_2)\n",
    "        self.recognition_net = Encoder(z_dim, hidden_1, hidden_2)\n",
    "\n",
    "    def model(self, xs, ys=None):\n",
    "        # register this pytorch module and all of its submodules with pyro\n",
    "        pyro.module(\"generation_net\", self)\n",
    "        batch_size = xs.shape[0]\n",
    "\n",
    "        with pyro.plate(\"data\"):\n",
    "            # Prior network uses the baseline predictions as an initial guess.\n",
    "            # This is the generative process with recurrent connection.\n",
    "            with torch.no_grad():\n",
    "                # this ensures that the training process does not change the baseline network\n",
    "                # view(ns) method returns a tensor with a new shape \"ns\"\n",
    "                y_hat = self.baseline_net(xs).view(xs.shape)\n",
    "            \n",
    "            # sample handwriting style z from the prior distribution, which is modulated by the input xs (x)\n",
    "            prior_loc, prior_scale = self.prior_net(xs, y_hat)\n",
    "            # recall that to_event will force the z's to be learned as one MVN rather than separate normals\n",
    "            zs = pyro.sample('z', dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # the output y is generated from the distribution pθ(y|x, z)\n",
    "            loc = self.generation_net(zs)\n",
    "\n",
    "            if ys is not None:\n",
    "                # In training, we will only sample in the masked image\n",
    "                mask_loc = loc[(xs == -1).view(-1, 784)].view(batch_size, -1)\n",
    "                mask_ys = ys[xs == -1].view(batch_size, -1)\n",
    "                pyro.sample('y', dist.Bernoulli(mask_loc).to_event(1), obs=mask_ys)\n",
    "            else:\n",
    "                # In testing, no need to sample: the output is already a\n",
    "                # probability in [0, 1] range, which better represent pixel\n",
    "                # values considering grayscale. If we sample, we will force\n",
    "                # each pixel to be  either 0 or 1, killing the grayscale\n",
    "                pyro.deterministic('y', loc.detach())\n",
    "        \n",
    "        # return the loc so we can visualize it later\n",
    "        return loc\n",
    "\n",
    "    # now we define the guide. \n",
    "    # at training time this is the recognition_net q(z|y,x).\n",
    "    # at testing time we do not have ys, so we use the prior network\n",
    "    def guide(self, xs, ys=None):\n",
    "        with pyro.plate(\"data\"):\n",
    "            if ys is None:\n",
    "                # at inference time, ys is not provided. In that case,\n",
    "                # the model uses the prior network\n",
    "                y_hat = self.baseline_net(xs).view(xs.shape)\n",
    "                loc, scale = self.prior_net(xs, y_hat)\n",
    "            else:\n",
    "                # at training time, uses the variational distribution\n",
    "                # q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "                loc, scale = self.recognition_net(xs, ys)\n",
    "\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch_pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
