{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch Lighting\n",
    "\n",
    "PyTorch Lightning is a wrapper around PyTorch that is very popular because it simplifies a lot of things:\n",
    "\n",
    "* Removes the need for a lot of boilerplate code in the training loop, such as tracking progress, computing metrics, backpropagating and resetting the gradient, etc\n",
    "* Makes it easy to access and use multiple GPUs \n",
    "* Makes it easy to save a model, check its performance, and train it for further epochs if needed\n",
    "\n",
    "We will introduce PyTorch lightning model classes, which differ a bit from vanilla PyTorch, and show how to use it for MNIST classification (compare to car_mnist_projects.ipynb).\n",
    "\n",
    "We will also introduce TensorBoard, a visualization tool for assessing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "from torchmetrics import __version__ as torchmetrics_version\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an MLP implemented as a PyTorch lightning (PL) module\n",
    "# PyTorch lightning recognizes several class method names; see comments below\n",
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, image_shape=(1, 28, 28), hidden_units=(32, 16)):\n",
    "        super().__init__()  # note here we inherit lightning module class, not nn.module\n",
    "\n",
    "        # new PL attributes: Accuracy() from torchmetrics automatically computes model Accuracy\n",
    "        if parse_version(torchmetrics_version) > parse_version(\"0.8\"):\n",
    "            self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            self.valid_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        else:\n",
    "            self.train_acc = Accuracy()\n",
    "            self.valid_acc = Accuracy()\n",
    "            self.test_acc = Accuracy()\n",
    "\n",
    "        # the neural network model itself will be similar to the one used in the MNIST example in car_mnist_projects.ipynb \n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2] \n",
    "        all_layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units: \n",
    "            layer = nn.Linear(input_size, hidden_unit) \n",
    "            all_layers.append(layer) \n",
    "            all_layers.append(nn.ReLU()) \n",
    "            input_size = hidden_unit \n",
    " \n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward pass is very simple given our defined self.model\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    # PL recognizes a function called training_step that tells it what actions to perform to train on a batch of data\n",
    "    # note that now training is defined inside the class, rather than as separate code\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)  # computes forward pass and returns output logits (I think?)\n",
    "        loss = nn.functional.cross_entropy(logits, y)  # loss function\n",
    "        preds = torch.argmax(logits, dim=1)   # make predictions\n",
    "        self.train_acc.update(preds, y)  # updated accuracy\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)  # log the training loss\n",
    "        return loss\n",
    "    \n",
    "    # similarly define test_step, defining what is done in a test step on a batch\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    # again, similarly define validation step. this is only done at specific times, like the end of an epoch.\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_acc.update(preds, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    # Conditionally define epoch end methods based on PyTorch Lightning version\n",
    "    # The epoch end methods run at the end of each epoch (the step ones run for each batch)\n",
    "    # Here we just use the Accuracy() metric from self.train_acc, etc, to compute the accuracy\n",
    "    #    for the whole epoch given the accuracy values accumulated at each step.\n",
    "    if parse_version(pl.__version__) >= parse_version(\"2.0\"):\n",
    "        # For PyTorch Lightning 2.0 and above\n",
    "        def on_training_epoch_end(self):\n",
    "            self.log(\"train_acc\", self.train_acc.compute())\n",
    "            self.train_acc.reset()\n",
    "\n",
    "        def on_validation_epoch_end(self):\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.valid_acc.reset()\n",
    "\n",
    "        def on_test_epoch_end(self):\n",
    "            self.log(\"test_acc\", self.test_acc.compute())\n",
    "            self.test_acc.reset()\n",
    "\n",
    "    else:\n",
    "        # For PyTorch Lightning < 2.0\n",
    "        def training_epoch_end(self, outs):\n",
    "            self.log(\"train_acc\", self.train_acc.compute())\n",
    "            self.train_acc.reset()\n",
    "\n",
    "        def validation_epoch_end(self, outs):\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.valid_acc.reset()\n",
    "\n",
    "        def test_epoch_end(self, outs):\n",
    "            self.log(\"test_acc\", self.test_acc.compute())\n",
    "            self.test_acc.reset()\n",
    "    \n",
    "    # PL recognizes the method \"configure_optimizers\" to define our optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataModule\n",
    "\n",
    "PL has a way for you to specify a class for custom data downloading and defining data loaders. Like with the training code, this makes everything more organized by defining a class rather than having free-floating code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    " \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path='../datasets/'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        MNIST(root=self.data_path, download=True) \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # stage is either 'fit', 'validate', 'test', or 'predict'\n",
    "        # here note relevant\n",
    "        mnist_all = MNIST( \n",
    "            root=self.data_path,\n",
    "            train=True,\n",
    "            transform=self.transform,  \n",
    "            download=False\n",
    "        ) \n",
    "\n",
    "        self.train, self.val = random_split(\n",
    "            mnist_all, [55000, 5000], generator=torch.Generator().manual_seed(1)\n",
    "        )\n",
    "\n",
    "        self.test = MNIST( \n",
    "            root=self.data_path,\n",
    "            train=False,\n",
    "            transform=self.transform,  \n",
    "            download=False\n",
    "        ) \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's simple to define a data loader for the MNIST data\n",
    "torch.manual_seed(1) \n",
    "mnist_dm = MnistDataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning Trainer Class\n",
    "\n",
    "PL has a Trainer class that actually trains your model class on your data loader. It is simple to run given that you have specified the above classes, obviating a lot of biolerplate code like zeroing out gradients. It also makes it easy to specify multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pyro_pytorch_venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pyro_pytorch_venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:45<00:00, 18.84it/s, v_num=0, train_loss=0.0912, valid_loss=0.161, valid_acc=0.953]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:45<00:00, 18.84it/s, v_num=0, train_loss=0.0912, valid_loss=0.161, valid_acc=0.953]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "mnistclassifier = MultiLayerPerceptron()\n",
    "\n",
    "# callbacks are a PyTorch lightning feature that allow us to define extra code that\n",
    "#    we want to execute during training. we can define any custom function here.\n",
    "# the idea is to keep our core PL module code clean, with bells and whistles as callbacks.\n",
    "# in our case, we will use PL's built-in ModelCheckpoint function to save the model\n",
    "#    with the best validation accuracy in each epoch, in case we start to overfit and\n",
    "#    it goes down at some point.\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_acc\")] # save top 1 model\n",
    "\n",
    "if torch.cuda.is_available():  # if you have GPUs\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks, devices=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks)\n",
    "\n",
    "trainer.fit(model=mnistclassifier, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pytorch_tb\\ch13\\lightning_logs\\version_0\\checkpoints\\epoch=8-step=7740.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pytorch_tb\\ch13\\lightning_logs\\version_0\\checkpoints\\epoch=8-step=7740.ckpt\n",
      "c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pyro_pytorch_venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:02<00:00, 74.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9599000215530396     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.13700783252716064    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9599000215530396    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.13700783252716064   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.13700783252716064, 'test_acc': 0.9599000215530396}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we evaluate the model on the test data using our trainer\n",
    "trainer.test(model=mnistclassifier, datamodule=mnist_dm, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating/Visualizing the model using TensorBoard\n",
    "\n",
    "TensorBoard is a graphical software package for visualizing and analyzing model performance. It can be run from the command line and pop up in the browser, or directly in Jupyter notebooks. We'll show how to use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b556e4335abbada4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b556e4335abbada4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard\n",
    "# by default PL puts performance logs in lightning_logs, so we pull performance data from there\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n",
    "# NOTE: has issues with VSCode, can deal with this later\n",
    "# Run in browser via command line, e.g. tensorboard --logdir .\\pytorch_tb\\ch13\\lightning_logs\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming training from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pyro_pytorch_venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pytorch_tb\\ch13\\lightning_logs\\version_0\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pyro_pytorch_venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pyro_pytorch_venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 860/860 [00:47<00:00, 17.98it/s, v_num=1, train_loss=0.0392, valid_loss=0.159, valid_acc=0.954] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 860/860 [00:47<00:00, 17.98it/s, v_num=1, train_loss=0.0392, valid_loss=0.159, valid_acc=0.954]\n"
     ]
    }
   ],
   "source": [
    "# here we resume from a desired checkpoint and set max_epochs=15.\n",
    "# this trains an additional 5 epochs since we previously did 10.\n",
    "path = 'lightning_logs/version_0/checkpoints/epoch=8-step=7740.ckpt'\n",
    "\n",
    "model = MultiLayerPerceptron.load_from_checkpoint(path)\n",
    "\n",
    "if torch.cuda.is_available(): # if you have GPUs\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5, callbacks=callbacks, devices=1\n",
    "    )\n",
    "else:\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5, callbacks=callbacks\n",
    "    )\n",
    "\n",
    "trainer.fit(model=model, datamodule=mnist_dm)\n",
    "\n",
    "\n",
    "# NOTE: this is the book's original code, but the new PL appears to not have resume_from_checkpoint\n",
    "# so i did it my own way, looking at https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html\n",
    "#if torch.cuda.is_available(): # if you have GPUs\n",
    "#    trainer = pl.Trainer(\n",
    "#        max_epochs=15, callbacks=callbacks, resume_from_checkpoint=path, gpus=1\n",
    "#    )\n",
    "#else:\n",
    "#    trainer = pl.Trainer(\n",
    "#        max_epochs=15, callbacks=callbacks, resume_from_checkpoint=path\n",
    "#    )\n",
    "\n",
    "#trainer.fit(model=mnistclassifier, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-visualize in tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pyro_pytorch_venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 100.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9603999853134155     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1347116380929947     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9603999853134155    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1347116380929947    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.1347116380929947, 'test_acc': 0.9603999853134155}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-evaluate test data\n",
    "trainer.test(model=model, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pytorch_tb\\ch13\\lightning_logs\\version_0\\checkpoints\\epoch=2-step=2580.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at c:\\Users\\natha\\Documents\\pyro_pytorch_tutorials\\pytorch_tb\\ch13\\lightning_logs\\version_0\\checkpoints\\epoch=2-step=2580.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:02<00:00, 70.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.960099995136261     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.13509604334831238    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.960099995136261    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.13509604334831238   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.13509604334831238, 'test_acc': 0.960099995136261}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the 'best' model on the validation data (via our callback)\n",
    "trainer.test(model=model, datamodule=mnist_dm, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PL saves the model automatically for us, so we can easily load this module from a checkpoint for later re-use\n",
    "path = \"lightning_logs/version_0/checkpoints/epoch=2-step=2580.ckpt\"\n",
    "model2 = MultiLayerPerceptron.load_from_checkpoint(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro_pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
