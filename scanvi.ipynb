{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scANVI Pyro Tutorial\n",
    "\n",
    "This notebook follows the Pyro tutorial on implementing a bare-bones version of the scANVI model: https://pyro.ai/examples/scanvi.html\n",
    "\n",
    "scANVI (https://doi.org/10.15252/msb.20209620) is a method based on conditional variational autoencoders (CVAEs) that learns cell state representations from single-cell RNA-seq data. It is a semi-supervised method that uses cell type labels when available. In this notebook, we will approximately reproduce Figure 6 in the scANVI paper.\n",
    "\n",
    "Note that the notation used below is somewhat different than that from scANVI. Below, \"y\" represents the cell class label and \"z1\" is the distribution describing cell characteristics within that label; the corresponding variables are called \"c\" and \"u\" respectively in scANVI. \"z2\" below is \"z\" in scANVI, and rather than elaborate the rest of the scANVI variables, the expression level \"x\" (same in both) is drawn directly as a zero-inflated negative binomial. (scANVI elaborates the underlying gamma and poisson variables and what they represent.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and data preprocessing\n",
    "\n",
    "We use the scvi-tools package to download some PBMC scRNA-seq data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)  # for continuous integration tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various import statements\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softplus, softmax\n",
    "from torch.distributions import constraints\n",
    "from torch.optim import Adam\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions.util import broadcast_shape\n",
    "from pyro.optim import MultiStepLR\n",
    "from pyro.infer import SVI, config_enumerate, TraceEnum_ELBO\n",
    "from pyro.contrib.examples.scanvi_data import get_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Downloading file at data/PurifiedPBMCDataset.h5ad                                                         \n",
      "Downloading...: 157054it [00:04, 35066.22it/s]                              \n"
     ]
    }
   ],
   "source": [
    "# Download and pre-process data\n",
    "batch_size = 100\n",
    "if not smoke_test:\n",
    "    # dataloader, num_genes, l_mean, l_scale, anndata = get_data(dataset='pbmc', cuda=True, batch_size=batch_size)\n",
    "    dataloader, num_genes, l_mean, l_scale, anndata = get_data(dataset='pbmc', cuda=False, batch_size=batch_size)\n",
    "else:\n",
    "    dataloader, num_genes, l_mean, l_scale, anndata = get_data(dataset='mock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count data matrix shape: torch.Size([20000, 21932])\n",
      "Mean counts per cell: 1418.6\n",
      "Number of labeled cells: 200\n"
     ]
    }
   ],
   "source": [
    "# get some basic info about the data\n",
    "print(\"Count data matrix shape:\", dataloader.data_x.shape)\n",
    "print(\"Mean counts per cell: {:.1f}\".format(dataloader.data_x.sum(-1).mean().item()))\n",
    "print(\"Number of labeled cells:\", dataloader.num_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions for reshaping tensors and \n",
    "#   making fully-connected neural networks (see scANVI paper to see where these are used)\n",
    "\n",
    "# Helper for making fully-connected neural networks\n",
    "def make_fc(dims):\n",
    "    layers = []\n",
    "    for in_dim, out_dim in zip(dims, dims[1:]):\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        layers.append(nn.BatchNorm1d(out_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers[:-1])  # Exclude final ReLU non-linearity\n",
    "\n",
    "# Splits a tensor in half along the final dimension\n",
    "def split_in_half(t):\n",
    "    return t.reshape(t.shape[:-1] + (2, -1)).unbind(-2)\n",
    "\n",
    "# Helper for broadcasting inputs to neural net\n",
    "def broadcast_inputs(input_args):\n",
    "    shape = broadcast_shape(*[s.shape[:-1] for s in input_args]) + (-1,)\n",
    "    input_args = [s.expand(shape) for s in input_args]\n",
    "    return input_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and guide sketches\n",
    "\n",
    "Before specifying the full model, we write some code to illustrate its high-level structure. Refer to the link above for more details.\n",
    "\n",
    "We also give a sketch of the guide -- the variational distribution of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this is only a sketch and will not run, since things like z2_decoder and x_decoder are not defined\n",
    "def model_sketch(x, y=None):\n",
    "    # This gene-level parameter (theta) modulates the variance of the observation distribution for our vector of counts x\n",
    "    # It is the gamma variance parameter and controls the level of over-dispersion in the ZINB distribution of x\n",
    "    theta = pyro.param(\"inverse_dispersion\", 10.0 * torch.ones(num_genes), constraint=constraints.positive)\n",
    "\n",
    "    # This plate statement encodes that each datapoint (i.e. cell count vector x_i)\n",
    "    #   is conditionally independent given its own latent variables.\n",
    "    with pyro.plate(\"batch\", len(x)):\n",
    "        # Define a unit normal prior for z1 (aka \"u\", the cell specific params conditional on cell type y)\n",
    "        # Remember, to_event(1) causes these to be sampled all at once as an MVN with identity covariance\n",
    "        z1 = pyro.sample(\"z1\", dist.Normal(0, torch.ones(latent_dim)).to_event(1))\n",
    "\n",
    "        # Define a uniform categorical prior for y (cell type).\n",
    "        # Note that (via obs=y) if y is None (i.e. y is unobserved) they y will be sampled; otherwise y will be treated as observed (via obs=y).\n",
    "        y = pyro.sample(\"y\", dist.OneHotCategorical(logits=torch.zeros(num_labels)), obs=y)\n",
    "\n",
    "        # pass z1 and y to the z2 decoder neural network, which \"decodes\" these latents to generate z2, \n",
    "        #   the cell-specific params (z in original scANVI)\n",
    "        z2_loc, z2_scale = z2_decoder(z1, y)\n",
    "        # Define the prior distribution for z2. The parameters of this distribution depend on both z1 and y.\n",
    "        z2 = pyro.sample(\"z2\", dist.Normal(z2_loc, z2_scale).to_event(1))\n",
    "\n",
    "        # Define a LogNormal prior distribution for log count variable l (capturing library size, capture efficiency, etc)\n",
    "        l = pyro.sample(\"l\", dist.LogNormal(l_loc, l_scale).to_event(1))\n",
    "\n",
    "        # We now construct the observation distribution. To do this we first pass z2 to the x decoder neural network,\n",
    "        #   which \"decodes\" the latent cell state z2 into the ZINB-distributed observed counts x.\n",
    "        gate_logits, mu = x_decoder(z2)\n",
    "        # Using the outputs of the neural network we can define the parameters\n",
    "        # of our ZINB observation distribution.\n",
    "        # Note that by construction mu is normalized (i.e. mu.sum(-1) == 1) and the\n",
    "        # total scale of counts for each cell is determined by the latent variable â„“.\n",
    "        # That is, `l * mu` is a G-dimensional vector of mean gene counts.\n",
    "        nb_logits = (l * mu).log() - theta.log()\n",
    "        x_dist = dist.ZeroInflatedNegativeBinomial(gate_logits=gate_logits, total_count=theta, logits=nb_logits)\n",
    "\n",
    "        # Observe the datapoint x using the observation distribution x_dist\n",
    "        pyro.sample(\"x\", x_dist.to_event(1), obs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sketch of the guide specifying the variational distribution; again, it will not run since some things are undefined\n",
    "def guide_sketch(self, x, y=None):\n",
    "    # This plate statement matches the plate in the model\n",
    "    with pyro.plate(\"batch\", len(x)):\n",
    "        # We pass the observed count vector x to an encoder network that generates the paramaters we use to define\n",
    "        #   the variational distributions for the latent variables z2 (cell state parameters) and l (size factors). \n",
    "        z2_loc, z2_scale, l_loc, l_scale = z2l_encoder(x)\n",
    "        pyro.sample(\"l\", dist.LogNormal(l_loc, l_scale).to_event(1))\n",
    "        z2 = pyro.sample(\"z2\", dist.Normal(z2_loc, z2_scale).to_event(1))\n",
    "\n",
    "        # We only need to specify a variational distribution over y if y is unobserved\n",
    "        if y is None:\n",
    "            # We use the \"classifier\" neural netowrk to turn the latent \"code\" z2 into \n",
    "            #   logits that we can use to specify a distribution over y.\n",
    "            y_logits = classifier(z2)\n",
    "            y_dist = dist.OneHotCategorical(logits=y_logits)\n",
    "            y = pyro.sample(\"y\", y_dist)\n",
    "        \n",
    "        # Finally we generate the parameters for the z1 distribution by passing z2 and y through an encoder neural network z1_encoder.\n",
    "        z1_loc, z1_scale = z1_encoder(z2, y)\n",
    "        pyro.sample(\"z1\", dist.Normal(z1_loc, z1_scale).to_event(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder networks\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scANVI model\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting / Visualizing Results\n",
    "todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch_pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
