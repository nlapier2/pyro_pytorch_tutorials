{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders in PyTorch and Pyro\n",
    "\n",
    "This notebook follows the Pyro introductory tutorial on building variational autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup: library imports, smoke test, data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pyro.contrib.examples.util import MNIST\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pyro.__version__.startswith('1.9.1')\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be experimenting on the MNIST dataset, which PyTorch has built-in dataloaders for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data loaders to load and batch MNIST data\n",
    "def setup_data_loaders(batch_size=128, use_cuda=False):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = MNIST(root=root, train=True, transform=trans,\n",
    "                      download=download)\n",
    "    test_set = MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder modules in PyTorch\n",
    "\n",
    "The decoder module takes in the latent layer z and applies a neural network to decode it into the original space x. Here we will use a simple network with two linear layers.\n",
    "\n",
    "The encoder module takes the input data and encodes it into the latent space. In a VAE, this is the variational family q(z|x), which will approximate the posterior p(z|x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # set up the two linear layers\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, 784)  # 784 = # of pixels in flattened MNIST images (28x28)\n",
    "        # set up the non-linear activation functions\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # define the forward computation on the latent z to the output\n",
    "        # first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        # return parameter for the output Bernoulli\n",
    "        # each is of size batch_size * 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        return loc_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # set up the three linear layers we will use to encode the input\n",
    "        # remember that these are encoding our variational parameters, the\n",
    "        #   mean and variance of the normal distribution (z_loc and z_scale)\n",
    "        # fc21 will be used for the mean and fc22 for the variance\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        # set up the non-linear activation function\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # define the forward computation on the image x\n",
    "        # first shape the mini-batch to flatten pixels\n",
    "        x = x.reshape(-1, 784)\n",
    "        # then compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        # then return a mean vector and a positive square root covariance,\n",
    "        #   each of size batch_size * z_dim\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return(z_loc, z_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Model and Guide\n",
    "\n",
    "Now we define the probabilistic model p(x|z)p(z) for our VAE. We then define the guide (variational family). The model is what will be learned by the decoder (recovering z from x) and the guide will be learned by the encoder (i.e. the variational family q(z|x)). We define our VAE PyTorch module with these components. \n",
    "\n",
    "It also contains a helper function for reconstructing images, i.e. taking an input image and then running it through the encoder and then decoder to reconstruct the image. This will be used for training and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our VAE class with the model and guide\n",
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional and we use 400 hidden units\n",
    "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks, using the classes defined above\n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda here puts the encoder and decoder parameters into gpu memory\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        # register PyTorch module 'decoder' with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)  # the first arg is what we name the module\n",
    "        with pyro.plate(\"data\", x.shape[0]):  \n",
    "            # pyro.plate designates independence among the samples in x\n",
    "            # setup hyperparameters for prior p(z)\n",
    "            # the new_zeros and new_ones ensure that the created tensors are on the same gpu device\n",
    "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "            # sample from prior p(z) (value will be sampled by guide when computing the ELBO)\n",
    "            # to_event(1) declares the dimensions of z as dependent, so sampled from an MVN\n",
    "            #   with diagonal covarance. this is to avoid needing another pyro.plate call. \n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # decode the latent code z, using our decoder, i.e. p(x|z)\n",
    "            loc_img = self.decoder(z)\n",
    "            # score against the actual images (how close is our reconstruction to the original?)\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
    "\n",
    "    # define the guide (i.e. variationsl distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        # register PyTorch module 'encoder' with Pyro (again, \"encoder\" is the name we assign)\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder(x)\n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "    \n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now we create an instance of our VAE module, define an optimizer and our training and test functions, and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = False\n",
    "NUM_EPOCHS = 1 if smoke_test else 5\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, _ in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to our training loop, but calls svi.evaluate_loss instead of svi.step\n",
    "#   because a gradient shoud not be computed\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x, _ in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 193.8537\n",
      "[epoch 000] average test loss: 158.7788\n",
      "[epoch 001]  average training loss: 148.1595\n",
      "[epoch 002]  average training loss: 133.7032\n",
      "[epoch 003]  average training loss: 125.0177\n",
      "[epoch 004]  average training loss: 119.5332\n"
     ]
    }
   ],
   "source": [
    "# initialize data loaders\n",
    "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup to optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Here we plot some reconstructed images from our model, and also plot a tSNE of our latent space, with colors corresponding to different classes. The latter helps us see if our embedded space is capturing differences in the classes, i.e. if it has captured latent structure in the data, as we had hoped.\n",
    "\n",
    "We will use some code from: https://github.com/pyro-ppl/pyro/blob/daea9a65ac6aefabce110e0f3a79c483138c3d08/examples/vae/utils/vae_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot code from pyro github\n",
    "# Copyright (c) 2017-2019 Uber Technologies, Inc.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import torch\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import visdom\n",
    "\n",
    "\n",
    "def plot_conditional_samples_ssvae(ssvae, visdom_session):\n",
    "    \"\"\"\n",
    "    This is a method to do conditional sampling in visdom\n",
    "    \"\"\"\n",
    "    vis = visdom_session\n",
    "    ys = {}\n",
    "    for i in range(10):\n",
    "        ys[i] = torch.zeros(1, 10)\n",
    "        ys[i][0, i] = 1\n",
    "    xs = torch.zeros(1, 784)\n",
    "\n",
    "    for i in range(10):\n",
    "        images = []\n",
    "        for rr in range(100):\n",
    "            # get the loc from the model\n",
    "            sample_loc_i = ssvae.model(xs, ys[i])\n",
    "            img = sample_loc_i[0].view(1, 28, 28).cpu().data.numpy()\n",
    "            images.append(img)\n",
    "        vis.images(images, 10, 2)\n",
    "\n",
    "\n",
    "def plot_llk(train_elbo, test_elbo):\n",
    "    Path(\"vae_results\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    df1 = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": train_elbo.keys(),\n",
    "            \"ELBO\": [-val for val in train_elbo.values()],\n",
    "            \"dataset\": \"Train\",\n",
    "        }\n",
    "    )\n",
    "    df2 = pd.DataFrame(\n",
    "        {\n",
    "            \"Epoch\": test_elbo.keys(),\n",
    "            \"ELBO\": [-val for val in test_elbo.values()],\n",
    "            \"dataset\": \"Test\",\n",
    "        }\n",
    "    )\n",
    "    df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "    # Create the FacetGrid with scatter plot\n",
    "    g = sns.FacetGrid(df, height=4, aspect=1.5, hue=\"dataset\")\n",
    "    g.map(sns.scatterplot, \"Epoch\", \"ELBO\")\n",
    "    g.map(sns.lineplot, \"Epoch\", \"ELBO\", linestyle=\"--\")\n",
    "    g.ax.yaxis.get_major_locator().set_params(integer=True)\n",
    "    g.add_legend()\n",
    "    plt.savefig(\"./vae_results/test_elbo_vae.png\")\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "def plot_vae_samples(vae, visdom_session):\n",
    "    vis = visdom_session\n",
    "    x = torch.zeros([1, 784])\n",
    "    for i in range(10):\n",
    "        images = []\n",
    "        for rr in range(100):\n",
    "            # get loc from the model\n",
    "            sample_loc_i = vae.model(x)\n",
    "            img = sample_loc_i[0].view(1, 28, 28).cpu().data.numpy()\n",
    "            images.append(img)\n",
    "        vis.images(images, 10, 2)\n",
    "\n",
    "\n",
    "def mnist_test_tsne(vae=None, test_loader=None):\n",
    "    \"\"\"\n",
    "    This is used to generate a t-sne embedding of the vae\n",
    "    \"\"\"\n",
    "    name = \"VAE\"\n",
    "    data = test_loader.dataset.test_data.float()\n",
    "    mnist_labels = test_loader.dataset.test_labels\n",
    "    z_loc, z_scale = vae.encoder(data)\n",
    "    plot_tsne(z_loc, mnist_labels, name)\n",
    "\n",
    "\n",
    "def mnist_test_tsne_ssvae(name=None, ssvae=None, test_loader=None):\n",
    "    \"\"\"\n",
    "    This is used to generate a t-sne embedding of the ss-vae\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"SS-VAE\"\n",
    "    data = test_loader.dataset.test_data.float()\n",
    "    mnist_labels = test_loader.dataset.test_labels\n",
    "    z_loc, z_scale = ssvae.encoder_z([data, mnist_labels])\n",
    "    plot_tsne(z_loc, mnist_labels, name)\n",
    "\n",
    "\n",
    "def plot_tsne(z_loc, classes, name):\n",
    "    model_tsne = TSNE(n_components=2, random_state=0)\n",
    "    z_states = z_loc.detach().cpu().numpy()\n",
    "    z_embed = model_tsne.fit_transform(z_states)\n",
    "    classes = classes.detach().cpu().numpy()\n",
    "    fig = plt.figure()\n",
    "    for ic in range(10):\n",
    "        ind_vec = np.zeros_like(classes)\n",
    "        ind_vec[:, ic] = 1\n",
    "        ind_class = classes[:, ic] == 1\n",
    "        color = plt.cm.Set1(ic)\n",
    "        plt.scatter(z_embed[ind_class, 0], z_embed[ind_class, 1], s=10, color=color)\n",
    "        plt.title(\"Latent Variable T-SNE per Class\")\n",
    "        fig.savefig(\"./vae_results/\" + str(name) + \"_embedding_\" + str(ic) + \".png\")\n",
    "    fig.savefig(\"./vae_results/\" + str(name) + \"_embedding.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some sample images\n",
    "# vis = visdom.Visdom()\n",
    "# plot_vae_samples(vae, vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tSNE plot\n",
    "# mnist_test_tsne(vae=vae, test_loader=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch_pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
